{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657b0259-09c4-4f30-802b-776012e9c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"/home/mviswanathsai/Downloads/names.txt\", \"r\")\n",
    "words = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161e0daa-2e7c-43ba-9e6b-e285f84cd450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 32033\n",
      "Train set size (80%): 25626\n",
      "Dev set size (10%): 3203\n",
      "Test set size (10%): 3204\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch # Just to ensure consistent results, use the PyTorch generator's seed\n",
    "\n",
    "# 1. Set a manual seed for reproducibility\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Use the Python random module, seeded from the PyTorch generator's state\n",
    "random.seed(g.initial_seed()) # Seeds Python's random with the PyTorch seed\n",
    "\n",
    "# 2. Shuffle the data\n",
    "# Shuffling the list in-place ensures the splits are random and not sequential\n",
    "random.shuffle(words)\n",
    "\n",
    "# 3. Calculate split sizes\n",
    "total_size = len(words)\n",
    "train_size = int(0.8 * total_size)  # 80%\n",
    "dev_size = int(0.1 * total_size)    # 10%\n",
    "# The remaining 10% goes to the test set to account for potential floating point errors\n",
    "test_size = total_size - train_size - dev_size\n",
    "\n",
    "# 4. Perform the slicing\n",
    "train_data = words[:train_size]\n",
    "dev_data = words[train_size : train_size + dev_size]\n",
    "test_data = words[train_size + dev_size :]\n",
    "\n",
    "print(f\"Total words: {total_size}\")\n",
    "print(f\"Train set size (80%): {len(train_data)}\")\n",
    "print(f\"Dev set size (10%): {len(dev_data)}\")\n",
    "print(f\"Test set size (10%): {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbde7de-4d64-4159-8013-a60f638f826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(alphabets)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e8067d-1ab2-4529-8850-9ee6140aec7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 11,  8,  ..., 18, 18, 25])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = [], [] \n",
    "\n",
    "for w in words:\n",
    "    w = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "\n",
    "xs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78d03123-963d-4b49-b06f-642d667ab731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "xenc = F.one_hot(xs, num_classes=27).float() \n",
    "\n",
    "(W[xs]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b67ee54d-313d-404e-99b9-a290e28378a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5469045639038086\n",
      "2.546839714050293\n",
      "2.5467753410339355\n",
      "2.546710968017578\n",
      "2.5466465950012207\n",
      "2.5465822219848633\n",
      "2.546517848968506\n",
      "2.5464537143707275\n",
      "2.5463900566101074\n",
      "2.546325922012329\n",
      "2.546261787414551\n",
      "2.5461981296539307\n",
      "2.5461344718933105\n",
      "2.5460708141326904\n",
      "2.546006917953491\n",
      "2.5459437370300293\n",
      "2.545880079269409\n",
      "2.545816659927368\n",
      "2.5457539558410645\n",
      "2.5456902980804443\n",
      "2.5456271171569824\n",
      "2.5455639362335205\n",
      "2.5455009937286377\n",
      "2.545438289642334\n",
      "2.5453755855560303\n",
      "2.5453126430511475\n",
      "2.545250177383423\n",
      "2.54518723487854\n",
      "2.5451247692108154\n",
      "2.54506254196167\n",
      "2.5450003147125244\n",
      "2.5449376106262207\n",
      "2.544875383377075\n",
      "2.544813394546509\n",
      "2.5447514057159424\n",
      "2.544689416885376\n",
      "2.5446271896362305\n",
      "2.544565439224243\n",
      "2.544503688812256\n",
      "2.5444419384002686\n",
      "2.5443801879882812\n",
      "2.544318914413452\n",
      "2.544257402420044\n",
      "2.5441958904266357\n",
      "2.5441346168518066\n",
      "2.5440735816955566\n",
      "2.5440120697021484\n",
      "2.5439510345458984\n",
      "2.5438899993896484\n",
      "2.5438292026519775\n",
      "2.5437679290771484\n",
      "2.5437071323394775\n",
      "2.5436463356018066\n",
      "2.543585777282715\n",
      "2.543525218963623\n",
      "2.543464422225952\n",
      "2.5434043407440186\n",
      "2.5433437824249268\n",
      "2.543283462524414\n",
      "2.5432231426239014\n",
      "2.5431628227233887\n",
      "2.543102979660034\n",
      "2.5430426597595215\n",
      "2.542982816696167\n",
      "2.5429232120513916\n",
      "2.542862892150879\n",
      "2.5428035259246826\n",
      "2.542743682861328\n",
      "2.5426840782165527\n",
      "2.5426244735717773\n",
      "2.542565107345581\n",
      "2.5425057411193848\n",
      "2.5424466133117676\n",
      "2.542387008666992\n",
      "2.542327880859375\n",
      "2.542268753051758\n",
      "2.5422098636627197\n",
      "2.5421507358551025\n",
      "2.5420918464660645\n",
      "2.5420329570770264\n",
      "2.541973829269409\n",
      "2.5419154167175293\n",
      "2.5418567657470703\n",
      "2.5417978763580322\n",
      "2.5417394638061523\n",
      "2.5416808128356934\n",
      "2.5416226387023926\n",
      "2.541564464569092\n",
      "2.541506052017212\n",
      "2.541447877883911\n",
      "2.5413899421691895\n",
      "2.5413320064544678\n",
      "2.541273832321167\n",
      "2.5412158966064453\n",
      "2.5411579608917236\n",
      "2.541100263595581\n",
      "2.5410425662994385\n",
      "2.540985107421875\n",
      "2.5409274101257324\n",
      "2.540869951248169\n",
      "2.5408124923706055\n",
      "2.540755033493042\n",
      "2.5406978130340576\n",
      "2.5406405925750732\n",
      "2.540583372116089\n",
      "2.5405261516571045\n",
      "2.540469169616699\n",
      "2.540412187576294\n",
      "2.5403552055358887\n",
      "2.5402987003326416\n",
      "2.5402417182922363\n",
      "2.540184736251831\n",
      "2.540128469467163\n",
      "2.540071725845337\n",
      "2.54001522064209\n",
      "2.5399587154388428\n",
      "2.539902448654175\n",
      "2.539846181869507\n",
      "2.5397896766662598\n",
      "2.539733648300171\n",
      "2.539677143096924\n",
      "2.539621591567993\n",
      "2.539565324783325\n",
      "2.5395095348358154\n",
      "2.5394532680511475\n",
      "2.539397716522217\n",
      "2.539341926574707\n",
      "2.5392861366271973\n",
      "2.5392305850982666\n",
      "2.539175033569336\n",
      "2.5391194820404053\n",
      "2.5390641689300537\n",
      "2.539008855819702\n",
      "2.5389533042907715\n",
      "2.538898229598999\n",
      "2.5388429164886475\n",
      "2.538787603378296\n",
      "2.5387327671051025\n",
      "2.5386781692504883\n",
      "2.5386228561401367\n",
      "2.5385680198669434\n",
      "2.53851318359375\n",
      "2.5384585857391357\n",
      "2.5384035110473633\n",
      "2.538349151611328\n",
      "2.538294553756714\n",
      "2.5382399559020996\n",
      "2.5381855964660645\n",
      "2.5381312370300293\n",
      "2.5380771160125732\n",
      "2.538022756576538\n",
      "2.537968635559082\n",
      "2.537914276123047\n",
      "2.53786039352417\n",
      "2.537806272506714\n",
      "2.537752151489258\n",
      "2.53769850730896\n",
      "2.537644863128662\n",
      "2.537590742111206\n",
      "2.537537097930908\n",
      "2.5374834537506104\n",
      "2.5374300479888916\n",
      "2.5373764038085938\n",
      "2.537322759628296\n",
      "2.537269353866577\n",
      "2.5372159481048584\n",
      "2.5371627807617188\n",
      "2.537109375\n",
      "2.5370566844940186\n",
      "2.537003517150879\n",
      "2.53695011138916\n",
      "2.5368974208831787\n",
      "2.536844253540039\n",
      "2.5367915630340576\n",
      "2.536738634109497\n",
      "2.5366859436035156\n",
      "2.536633014678955\n",
      "2.5365805625915527\n",
      "2.5365278720855713\n",
      "2.536475658416748\n",
      "2.5364229679107666\n",
      "2.5363705158233643\n",
      "2.536318302154541\n",
      "2.5362656116485596\n",
      "2.5362136363983154\n",
      "2.536161422729492\n",
      "2.536109447479248\n",
      "2.536057472229004\n",
      "2.5360052585601807\n",
      "2.5359535217285156\n",
      "2.5359013080596924\n",
      "2.5358498096466064\n",
      "2.5357978343963623\n",
      "2.5357463359832764\n",
      "2.5356943607330322\n",
      "2.5356431007385254\n",
      "2.5355911254882812\n",
      "2.5355401039123535\n",
      "2.5354886054992676\n",
      "2.5354371070861816\n"
     ]
    }
   ],
   "source": [
    "for k in range(200):\n",
    "    logits = W[xs] # log counts\n",
    "    counts = logits.exp() # counts, equivalent to what we had in N.\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data += -0.8* W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a57555-b263-4ead-bab3-b418c9f0da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexza.\n",
      "mogllurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n",
      "da.\n",
      "staiypucjalerigotai.\n",
      "miziellavo.\n",
      "ke.\n",
      "teda.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # log counts\n",
    "        counts = logits.exp() # counts, equivalent to what we had in N.\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0: \n",
    "            break\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429e260-ca11-41f4-aa6b-c9a4bffed1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
